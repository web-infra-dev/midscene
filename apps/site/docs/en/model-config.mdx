import TroubleshootingLLMConnectivity from './common/troubleshooting-llm-connectivity.mdx';

# Model Configuration

Midscene reads configuration from operating-system environment variables.

Midscene ships with OpenAI SDK support out of the box. That SDK constrains the request / response schema, and most model vendors (or deployment tools) already expose OpenAI-compatible endpoints, so you can swap providers freely.

This guide dives into the configuration parameters. If you are looking for model-selection guidance, read [Model strategy](./model-strategy).

## Quick start

Configuration snippets for common models.

### Doubao Seed vision models {#doubao-seed-vision}

Obtain an API key from [Volcano Engine](https://volcengine.com) and set:

```bash
MIDSCENE_MODEL_BASE_URL="https://ark.cn-beijing.volces.com/api/v3"
MIDSCENE_MODEL_API_KEY="...."
MIDSCENE_MODEL_NAME="ep-..." # Inference endpoint ID or model name from Volcano Engine
MIDSCENE_MODEL_FAMILY="doubao-vision"
```

### Qwen3-VL {#qwen3-vl}

Using Alibaba Cloud's `qwen3-vl-plus` as an example:

```bash
MIDSCENE_MODEL_BASE_URL="https://dashscope.aliyuncs.com/compatible-mode/v1"
MIDSCENE_MODEL_API_KEY="......"
MIDSCENE_MODEL_NAME="qwen3-vl-plus"
MIDSCENE_MODEL_FAMILY="qwen3-vl"
```

### Qwen2.5-VL {#qwen25-vl}

Using Alibaba Cloud's `qwen-vl-max-latest` as an example:

```bash
MIDSCENE_MODEL_BASE_URL="https://dashscope.aliyuncs.com/compatible-mode/v1"
MIDSCENE_MODEL_API_KEY="......"
MIDSCENE_MODEL_NAME="qwen-vl-max-latest"
MIDSCENE_MODEL_FAMILY="qwen2.5-vl"
```

### Gemini-3-Pro {#gemini-3-pro}

After requesting an API key from [Google Gemini](https://gemini.google.com/), configure:

```bash
MIDSCENE_MODEL_BASE_URL="https://generativelanguage.googleapis.com/v1beta/openai/"
MIDSCENE_MODEL_API_KEY="......"
MIDSCENE_MODEL_NAME="gemini-3.0-pro-preview" # Replace with the specific Gemini 3 Pro release name you are using
MIDSCENE_MODEL_FAMILY="gemini"
```

### UI-TARS {#ui-tars}

Use the deployed `doubao-1.5-ui-tars` on [Volcano Engine](https://volcengine.com):

```bash
MIDSCENE_MODEL_BASE_URL="https://ark.cn-beijing.volces.com/api/v3"
MIDSCENE_MODEL_API_KEY="...."
MIDSCENE_MODEL_NAME="ep-2025..." # Inference endpoint ID or model name from Volcano Engine
MIDSCENE_MODEL_FAMILY="vlm-ui-tars-doubao-1.5"
```

**About `MIDSCENE_MODEL_FAMILY`**

This variable selects the UI-TARS version. Supported values:
- `vlm-ui-tars` â€“ for the 1.0 release
- `vlm-ui-tars-doubao` â€“ for the 1.5 release deployed on Volcano Engine (equivalent to `vlm-ui-tars-doubao-1.5`)
- `vlm-ui-tars-doubao-1.5` â€“ for the 1.5 release deployed on Volcano Engine

:::tip

The legacy configurations `MIDSCENE_USE_VLM_UI_TARS=DOUBAO` or `MIDSCENE_USE_VLM_UI_TARS=1.5` are still supported but deprecated. Please migrate to `MIDSCENE_MODEL_FAMILY`.

Migration mapping:
- `MIDSCENE_USE_VLM_UI_TARS=1.0` â†’ `MIDSCENE_MODEL_FAMILY="vlm-ui-tars"`
- `MIDSCENE_USE_VLM_UI_TARS=1.5` â†’ `MIDSCENE_MODEL_FAMILY="vlm-ui-tars-doubao-1.5"`
- `MIDSCENE_USE_VLM_UI_TARS=DOUBAO` â†’ `MIDSCENE_MODEL_FAMILY="vlm-ui-tars-doubao"`

:::

### <del>`GPT-4o`</del>

Starting with version 1.0, Midscene no longer supports gpt series models as the default model. See [Model strategy](./model-strategy) for details.

## Ways to set environment variables

Midscene reads all configuration from environment variables. Below are common approaches, but feel free to adopt any method used in your project.

### Method 1: Set variables in the system

> The Midscene Chrome extension also accepts this `export KEY="value"` format.

```bash
# Replace with your own API key
export MIDSCENE_MODEL_API_KEY="sk-abcde..."
export MIDSCENE_MODEL_BASE_URL="https://.../compatible-mode/v1"
export MIDSCENE_MODEL_API_KEY="......"
export MIDSCENE_MODEL_NAME="qwen3-vl-plus"
```

### Method 2: Create a `.env` file

Create a `.env` file in the directory where you run the project. Midscene CLI tools load this file automatically.

```bash
MIDSCENE_MODEL_API_KEY="sk-abcdefghijklmnopqrstuvwxyz"
```

Keep in mind:
1. You do **not** need to prefix each line with `export`.
2. Only the Midscene CLI automatically reads this file. For the JavaScript SDK, load it manually as shown below.

### Method 3: Load variables via dotenv

[Dotenv](https://www.npmjs.com/package/dotenv) is a zero-dependency npm package that loads variables from `.env` into Node.js `process.env`.

Our [demo project](https://github.com/web-infra-dev/midscene-example) uses this method.

```bash
# install dotenv
npm install dotenv --save
```

Create a `.env` file in the project root and add (no `export` prefix):

```bash
MIDSCENE_MODEL_API_KEY="sk-abcdefghijklmnopqrstuvwxyz"
```

Import dotenv in your script; it will read `.env` automatically:

```typescript
import 'dotenv/config';
```

## Configuration items

### Required settings

| Name | Description |
|------|-------------|
| `MIDSCENE_MODEL_API_KEY` | Model API key, e.g., `"sk-abcd..."` |
| `MIDSCENE_MODEL_BASE_URL` | API endpoint URL, usually ending with a version (e.g., `/v1`); do not append `/chat/completion` here since the underlying sdk will add it automatically |
| `MIDSCENE_MODEL_NAME` | Model name |
| `MIDSCENE_MODEL_FAMILY` | Model family, determine the way of dealing with the coordinates |

### Advanced settings (optional)

| Name | Description |
|------|-------------|
| `MIDSCENE_MODEL_TIMEOUT` | Timeout for AI API calls in milliseconds (default intent). Defaults to OpenAI SDK default (10 minutes) |
| `MIDSCENE_INSIGHT_MODEL_TIMEOUT` | Timeout for Insight intent AI API calls in milliseconds. If not set, falls back to `MIDSCENE_MODEL_TIMEOUT` |
| `MIDSCENE_PLANNING_MODEL_TIMEOUT` | Timeout for Planning intent AI API calls in milliseconds. If not set, falls back to `MIDSCENE_MODEL_TIMEOUT` |
| `MIDSCENE_MODEL_MAX_TOKENS` | `max_tokens` for responses, default 2048 |
| `MIDSCENE_MODEL_HTTP_PROXY` | HTTP/HTTPS proxy, e.g., `http://127.0.0.1:8080` or `https://proxy.example.com:8080`. Takes precedence over `MIDSCENE_MODEL_SOCKS_PROXY` |
| `MIDSCENE_MODEL_SOCKS_PROXY` | SOCKS proxy, e.g., `socks5://127.0.0.1:1080` |
| `MIDSCENE_MODEL_INIT_CONFIG_JSON` | JSON blob that overrides the OpenAI SDK initialization config |
| `MIDSCENE_RUN_DIR` | Run artifact directory, like report and logs. Defaults to `midscene_run` in the current working directory; accepts absolute or relative paths |
| `MIDSCENE_PREFERRED_LANGUAGE` | Optional. Preferred response language. Defaults to `Chinese` if timezone is GMT+8, otherwise `English` |

> Note: Control replanning behavior with the agent option `replanningCycleLimit` (defaults to 20, or 40 for `vlm-ui-tars`), not with environment variables.

### Configure a dedicated Insight model (optional)

Set the following if the Insight intent needs a different model:

| Name | Description |
|------|-------------|
| `MIDSCENE_INSIGHT_MODEL_API_KEY` | API key |
| `MIDSCENE_INSIGHT_MODEL_BASE_URL` | API endpoint URL (omit the trailing `/chat/completion`) |
| `MIDSCENE_INSIGHT_MODEL_NAME` | Model name |
| `MIDSCENE_INSIGHT_MODEL_TIMEOUT` | Optional; timeout for Insight intent AI API calls in milliseconds |
| `MIDSCENE_INSIGHT_MODEL_HTTP_PROXY` | Optional; same effect as `MIDSCENE_MODEL_HTTP_PROXY` |
| `MIDSCENE_INSIGHT_MODEL_SOCKS_PROXY` | Optional; same effect as `MIDSCENE_MODEL_SOCKS_PROXY` |
| `MIDSCENE_INSIGHT_MODEL_INIT_CONFIG_JSON` | Optional; same effect as `MIDSCENE_MODEL_INIT_CONFIG_JSON` |

### Configure a dedicated Planning model (optional)

Set the following if the Planning intent needs a different model:

| Name | Description |
|------|-------------|
| `MIDSCENE_PLANNING_MODEL_API_KEY` | API key |
| `MIDSCENE_PLANNING_MODEL_BASE_URL` | API endpoint URL (omit the trailing `/chat/completion`) |
| `MIDSCENE_PLANNING_MODEL_NAME` | Model name |
| `MIDSCENE_PLANNING_MODEL_TIMEOUT` | Optional; timeout for Planning intent AI API calls in milliseconds |
| `MIDSCENE_PLANNING_MODEL_HTTP_PROXY` | Optional; same effect as `MIDSCENE_MODEL_HTTP_PROXY` |
| `MIDSCENE_PLANNING_MODEL_SOCKS_PROXY` | Optional; same effect as `MIDSCENE_MODEL_SOCKS_PROXY` |
| `MIDSCENE_PLANNING_MODEL_INIT_CONFIG_JSON` | Optional; same effect as `MIDSCENE_MODEL_INIT_CONFIG_JSON` |

### Debug logging switches (optional)

Enable the following variables to print richer debug logs. Regardless of the switches, logs are also saved under `./midscene_run/log`.

| Name | Description |
|------|-------------|
| `DEBUG=midscene:ai:profile:stats` | Prints model latency, token usage, etc., comma-separated for easier analysis |
| `DEBUG=midscene:ai:profile:detail` | Prints detailed token-usage logs |
| `DEBUG=midscene:ai:call` | Prints AI response details |
| `DEBUG=midscene:android:adb` | Prints Android adb command details |
| `DEBUG=midscene:*` | Prints every debug log |

### Still-compatible configs (not recommended)

The following environment variables are deprecated but still compatible. We recommend migrating to the new configuration approach.

#### Planning model configuration (deprecated)

| Name | Description | New approach |
|------|-------------|--------------|
| `MIDSCENE_USE_DOUBAO_VISION` | Deprecated. Enables Doubao vision model | Use `MIDSCENE_MODEL_FAMILY="doubao-vision"` |
| `MIDSCENE_USE_QWEN3_VL` | Deprecated. Enables Qwen3-VL model | Use `MIDSCENE_MODEL_FAMILY="qwen3-vl"` |
| `MIDSCENE_USE_QWEN_VL` | Deprecated. Enables Qwen2.5-VL model | Use `MIDSCENE_MODEL_FAMILY="qwen2.5-vl"` |
| `MIDSCENE_USE_GEMINI` | Deprecated. Enables Gemini model | Use `MIDSCENE_MODEL_FAMILY="gemini"` |
| `MIDSCENE_USE_VLM_UI_TARS` | Deprecated. Enables UI-TARS model | Use `MIDSCENE_MODEL_FAMILY="vlm-ui-tars*"` |

#### General configuration (deprecated)

| Name | Description | New approach |
|------|-------------|--------------|
| `OPENAI_API_KEY` | Deprecated but supported | Prefer `MIDSCENE_MODEL_API_KEY` |
| `OPENAI_BASE_URL` | Deprecated but supported | Prefer `MIDSCENE_MODEL_BASE_URL` |
| `MIDSCENE_OPENAI_INIT_CONFIG_JSON` | Deprecated but supported | Prefer `MIDSCENE_MODEL_INIT_CONFIG_JSON` |
| `MIDSCENE_OPENAI_HTTP_PROXY` | Deprecated but supported | Prefer `MIDSCENE_MODEL_HTTP_PROXY` |
| `MIDSCENE_OPENAI_SOCKS_PROXY` | Deprecated but supported | Prefer `MIDSCENE_MODEL_SOCKS_PROXY` |
| `OPENAI_MAX_TOKENS` | Deprecated but supported | Prefer `MIDSCENE_MODEL_MAX_TOKENS` |

## Configure settings via JavaScript

You can configure models for each agent in JavaScript. See the [API reference](./api) for details.

```typescript
const agent = new Agent(page, {
  // Configure via modelConfig
  modelConfig: {
    MIDSCENE_MODEL_TIMEOUT: '60000', // 60 seconds
    MIDSCENE_MODEL_NAME: 'qwen3-vl-plus',
    // For per-intent timeout configuration:
    MIDSCENE_INSIGHT_MODEL_TIMEOUT: '90000',
    MIDSCENE_PLANNING_MODEL_TIMEOUT: '120000',
    // ... other configurations
  }
});
```

<TroubleshootingLLMConnectivity />

## FAQ

### How can I monitor token usage?

Set `DEBUG=midscene:ai:profile:stats` to print usage and latency.

You can also find usage statistics inside the generated report files.


### Using LangSmith

LangSmith is a platform for debugging large language models. Midscene provides auto-integration support - just install the dependency and set environment variables.

**Step 1: Install dependency**

```bash
npm install langsmith
```

**Step 2: Set environment variables**

```bash
# Enable Midscene's LangSmith auto-integration
export MIDSCENE_LANGSMITH_DEBUG=1

# LangSmith configuration
export LANGCHAIN_API_KEY="your-langchain-api-key-here"
export LANGCHAIN_TRACING=true
export LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
# export LANGCHAIN_ENDPOINT="https://eu.api.smith.langchain.com" # If signed up in the EU region
```

After starting Midscene, you should see logs similar to:

```log
DEBUGGING MODE: langsmith wrapper enabled
```

For more details, see the [API documentation](./api#using-langsmith).

### Using Langfuse

Langfuse is another popular LLM observability platform. Integration is similar to LangSmith.

**Step 1: Install dependency**

```bash
npm install langfuse
```

**Step 2: Set environment variables**

```bash
# Enable Midscene's Langfuse auto-integration
export MIDSCENE_LANGFUSE_DEBUG=1

# Langfuse configuration
export LANGFUSE_PUBLIC_KEY="your-langfuse-public-key-here"
export LANGFUSE_SECRET_KEY="your-langfuse-secret-key-here"
export LANGFUSE_BASE_URL="https://cloud.langfuse.com" # ðŸ‡ªðŸ‡º EU region
# export LANGFUSE_BASE_URL="https://us.cloud.langfuse.com" # ðŸ‡ºðŸ‡¸ US region
```

After starting Midscene, you should see logs similar to:

```log
DEBUGGING MODE: langfuse wrapper enabled
```

For more details, see the [API documentation](./api#using-langfuse).
