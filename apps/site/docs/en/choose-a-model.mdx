import TroubleshootingLLMConnectivity from './common/troubleshooting-llm-connectivity.mdx';

# Choose a Model

Choose one of the following models, obtain the API key, complete the configuration, and you are ready to go. Choose the model that is easiest to obtain if you are a beginner.

## Adapted models for using Midscene.js

Midscene.js supports two types of models, which are:

### LLM models

Models that can understand text and image input. GPT-4o is this kind of model.

LLM models can only be used in web automation.

* [`GPT-4o`](#gpt-4o)
* [Other LLM models](#other-llm-models)

### VL models (‚ú® recommended)

Besides the ability to understand text and image input, VL(Visual-Language) models can also locate the target elements on the page. 

These models are recommended for UI automation since they can natively "see" the screenshot and return the coordinates of target elements on the page, which is much more reliable and efficient in complex scenarios.

VL models can be used in UI automation of any kind of interfaces.

These are the adapted VL models:

* [`Qwen-2.5-VL`](#qwen-25-vl)
* [`Doubao-1.5-thinking-vision-pro`](#doubao-15-thinking-vision-pro)
* [`Gemini-2.5-Pro`](#gemini-25-pro)
* [`UI-TARS`](#ui-tars)

If you want to see the detailed configuration of model services, see [Config Model and Provider](./model-provider).

## Models in Depth

<div id="gpt-4o"></div>
### `GPT-4o`

GPT-4o is a multimodal LLM by OpenAI, which supports image input. This is the default model for Midscene.js. When using GPT-4o, a step-by-step prompting is preferred.

The token cost of using GPT-4o is higher since Midscene needs to send some dom info and screenshot to the model, and it's not stable in complex scenarios.

**Config**

```bash
OPENAI_API_KEY="......"
OPENAI_BASE_URL="https://custom-endpoint.com/compatible-mode/v1" # optional, if you want an endpoint other than the default one from OpenAI.
MIDSCENE_MODEL_NAME="gpt-4o-2024-11-20" # optional. The default is "gpt-4o".
```

<div id="qwen-25-vl"></div>
### `Qwen-2.5-VL` on Openrouter or Alibaba Cloud

From 0.12.0 version, Midscene.js supports Qwen-2.5-VL-72B-Instruct model series.

Qwen-2.5-VL is an open-source model series published by Alibaba. It provides Visual Grounding ability, which can accurately return the coordinates of target elements on the page. When using it for interaction, assertion and query, it performs quite well. We recommend using the largest version (72B) for reliable output.

**config**

After applying for the API key on [Openrouter](https://openrouter.ai) you can use the following config:

```bash
OPENAI_BASE_URL="https://openrouter.ai/api/v1"
OPENAI_API_KEY="......"
MIDSCENE_MODEL_NAME="qwen/qwen2.5-vl-72b-instruct"
MIDSCENE_USE_QWEN_VL=1
```

Or from Alibaba Cloud:

```bash
OPENAI_BASE_URL="https://dashscope-intl.aliyuncs.com/compatible-mode/v1"
OPENAI_API_KEY="......"
MIDSCENE_MODEL_NAME="qwen-vl-max-latest"
MIDSCENE_USE_QWEN_VL=1
```

**Limitations when used in Midscene.js**

- **Not good at small icon recognition**: to recognize small icons, you may need to [enable the `deepThink` parameter](./blog-introducing-instant-actions-and-deep-think) and optimize the description, otherwise the recognition result may not be accurate.
- **Perform not that good on assertion**: we observed that it may not work as well as `GPT-4o` or `Doubao-1.5-thinking-vision-pro` on assertion. 

**Note about the model deployment on Aliyun.com**

‚Å†While the open-source version of Qwen-2.5-VL (72B) is named `qwen2.5-vl-72b-instruct`, there is also an enhanced and more stable version named `qwen-vl-max-latest` officially hosted on Aliyun.com. When using the `qwen-vl-max-latest` model on Aliyun, you will get larger context support and a much lower price (likely only 19% of the open-source version).

In short, if you want to use the Aliyun service, use `qwen-vl-max-latest`.

**Links**
- [Qwen 2.5 on ü§ó HuggingFace](https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct)
- [Qwen 2.5 on Github](https://github.com/QwenLM/Qwen2.5-VL)
- [Qwen 2.5 on Aliyun](https://bailian.console.aliyun.com/#/model-market/detail/qwen-vl-max-latest)
- [Qwen 2.5 on openrouter.ai](https://openrouter.ai/qwen/qwen2.5-vl-72b-instruct)

<div id="gemini-25-pro"></div>
### `Gemini-2.5-Pro` on Google Gemini

From 0.15.1 version, Midscene.js supports Gemini-2.5-Pro model. Gemini 2.5 Pro is a closed-source model provided by Google Cloud.

When using Gemini-2.5-Pro, you should use the `MIDSCENE_USE_GEMINI=1` config to turn on the Gemini-2.5-Pro mode. 

**Config**

After applying for the API key on [Google Gemini](https://gemini.google.com/), you can use the following config:

```bash
OPENAI_BASE_URL="https://generativelanguage.googleapis.com/v1beta/openai/"
OPENAI_API_KEY="......"
MIDSCENE_MODEL_NAME="gemini-2.5-pro-preview-05-06"
MIDSCENE_USE_GEMINI=1
```

**Links**
- [Gemini 2.5 on Google Cloud](https://cloud.google.com/gemini-api/docs/gemini-25-overview)

<div id="doubao-15-thinking-vision-pro"></div>

### `Doubao-1.5-thinking-vision-pro` on Volcano Engine

Doubao-1.5-thinking-vision-pro is a model provided by Volcano Engine. It works better on visual grounding and assertion.

**Config**

You can use the `Doubao-1.5-thinking-vision-pro` model on [Volcano Engine](https://volcengine.com). After obtaining an API key from Volcano Engine, you can use the following configuration:

```bash
OPENAI_BASE_URL="https://ark.cn-beijing.volces.com/api/v3" 
OPENAI_API_KEY="...."
MIDSCENE_MODEL_NAME="ep-..." # Inference endpoint ID or model name from Volcano Engine
MIDSCENE_USE_DOUBAO_VISION=1
```

**Links**
- [Doubao-1.5-thinking-vision-pro on Volcano Engine](https://www.volcengine.com/docs/82379/1536428)


<div id="ui-tars"></div>

### `UI-TARS` on  Volcano Engine

UI-TARS is an end-to-end GUI agent model based on VLM architecture. It solely perceives screenshots as input and performs human-like interactions (e.g., keyboard and mouse operations), achieving state-of-the-art performance on 10+ GUI benchmarks. UI-TARS is an open-source model, and provides different versions of size. 

When using UI-TARS, you can use target-driven style prompts, like "Login with user name foo and password bar", and it will plan the steps to achieve the goal.

**Config**

You can use the deployed `doubao-1.5-ui-tars` on  [Volcano Engine](https://volcengine.com).

```bash
OPENAI_BASE_URL="https://ark.cn-beijing.volces.com/api/v3" 
OPENAI_API_KEY="...."
MIDSCENE_MODEL_NAME="ep-2025..." # Inference endpoint ID or model name from Volcano Engine
MIDSCENE_USE_VLM_UI_TARS=DOUBAO
```

**Limitations**

- **Perform not good on assertion**: it may not work as well as GPT-4o and Qwen 2.5 on assertion and query.
- **Not stable on action path**: It may try different paths to achieve the goal, so the action path is not stable each time you call it.

**About the `MIDSCENE_USE_VLM_UI_TARS` config**

The `MIDSCENE_USE_VLM_UI_TARS` config is used to specify the UI-TARS version, use one of these values:
- `1.0` - for the model version `1.0`
- `1.5` - for the model version `1.5`
- `DOUBAO` - for the model deployed on Volcano Engine

**Links**
- [UI-TARS on ü§ó HuggingFace](https://huggingface.co/bytedance-research/UI-TARS-72B-SFT)
- [UI-TARS on Github](https://github.com/bytedance/ui-tars)
- [UI-TARS - Model Deployment Guide](https://juniper-switch-f10.notion.site/UI-TARS-Model-Deployment-Guide-17b5350241e280058e98cea60317de71)
- [UI-TARS on Volcengine](https://www.volcengine.com/docs/82379/1536429)


<div id="other-llm-models"></div>
## Choose other multimodal LLMs

Other models are also supported by Midscene.js. Midscene will use the same prompt and strategy as GPT-4o for these models. If you want to use other models, please follow these steps:

1. A multimodal model is required, which means it must support image input.
1. The larger the model, the better it works. However, it needs more GPU or money.
1. Find out how to to call it with an OpenAI SDK compatible endpoint. Usually you should set the `OPENAI_BASE_URL`, `OPENAI_API_KEY` and `MIDSCENE_MODEL_NAME`. Config are described in [Config Model and Provider](./model-provider).
1. If you find it not working well after changing the model, you can try using some short and clear prompt, or roll back to the previous model. See more details in [Prompting Tips](./prompting-tips).
1. Remember to follow the terms of use of each model and provider.
1. Don't include the `MIDSCENE_USE_VLM_UI_TARS` and `MIDSCENE_USE_QWEN_VL` config unless you know what you are doing.

**Config**

```bash
MIDSCENE_MODEL_NAME="....."
OPENAI_BASE_URL="......"
OPENAI_API_KEY="......"
```

For more details and sample config, see [Config Model and Provider](./model-provider).

## FAQ

### How can I check the model's token usage?

By setting `DEBUG=midscene:ai:profile:stats` in the environment variables, you can print the model's usage info and response time.

You can also see your model's usage info in the report file.

## More

* [Config Model and Provider](./model-provider)
* [Prompting Tips](./prompting-tips)

<TroubleshootingLLMConnectivity />