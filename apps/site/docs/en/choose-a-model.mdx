import TroubleshootingLLMConnectivity from './common/troubleshooting-llm-connectivity.mdx';

# Choose a model

## Using Vision Models for planning

The key to using AI models to drive UI automation lies in two aspects: accurately locating elements that need interaction, and planning reasonable operation paths. Among these, "accurately locating elements" is the most challenging task.

To accomplish element positioning, mainstream UI automation frameworks have two technical approaches:

* DOM + screenshot annotation: Extract the page's DOM structure in advance, combine it with screenshots for annotation, and have the model "select" elements from them.
* Pure vision: Complete all analysis work based on screenshots, meaning the model receives only images, no DOM, and no annotation information.

In the early days, Midscene supported both modes, allowing developers to choose freely. After dozens of version iterations, we found that the "pure vision" approach gradually demonstrated superiority:

* Industry-leading vision models (such as Qwen3-VL, Doubao Seed 1.6, etc.) perform stably enough to meet most business needs
* Can adapt to broader UI scenarios: The pure vision approach no longer depends on the UI rendering technology stack, whether it's Android, iOS, desktop applications, or `<canvas />` tags in browsers, as long as screenshots can be obtained, it can be understood and operated by AI
* Significant reduction in token usage: Without transmitting DOM, token usage is reduced by 80%, and runtime performance is faster. This makes the Agent cheaper and more responsive.
* A batch of excellent open-source VL models has emerged: For example, the Qwen3-VL model provides different sizes like 8b, 30b, etc., giving developers the opportunity to deploy privately on high-configured Macs and achieve good results.

Considering the development trend of vision understanding models, starting from version 1.0, Midscene only supports the pure vision model approach and no longer provides a "DOM extraction" compatibility mode.

## Model Configuration

Starting from version 1.0, Midscene.js recommends the following environment variable names:

- `MIDSCENE_MODEL_API_KEY` - API key (recommended)
- `MIDSCENE_MODEL_BASE_URL` - API endpoint URL (recommended)

For backward compatibility with existing OpenAI-compatible tooling, the following variables are still supported:

- `OPENAI_API_KEY` - API key (deprecated but still compatible)
- `OPENAI_BASE_URL` - API endpoint URL (deprecated but still compatible)

When both new and OpenAI-compatible variables are set, the new variables (`MIDSCENE_MODEL_*`) take precedence. Configuration examples in this document use the new names, but you don't need to migrate immediately if you're still on the legacy ones.

Midscene requires model service providers to provide OpenAI-style compatible interfaces.

You need to configure the following environment variables before use:

- `MIDSCENE_MODEL_BASE_URL` - API access address
- `MIDSCENE_MODEL_API_KEY` - API key
- `MIDSCENE_MODEL_NAME` - Model name


## Supported Vision Models

Midscene recommends the following models:
* [Qwen VL](#qwen3-vl-or-qwen-25-vl)
* [Doubao Vision Language Models](#doubao-vision)
* [`Gemini-2.5-Pro`](#gemini-25-pro)
* [`UI-TARS`](#ui-tars)
<div id="doubao-vision"></div>

### Doubao Vision Language Models (âœ¨ recommended)

These are vision language models provided by [Volcano Engine](https://volcengine.com). They perform excellently in complex scenarios for visual grounding and assertion:

* `Doubao-seed-1.6-vision` (latest version, most excellent)
* `Doubao-1.5-thinking-vision-pro`

**Configuration**

After obtaining an API key from [Volcano Engine](https://volcengine.com), you can use the following configuration:

```bash
MIDSCENE_MODEL_BASE_URL="https://ark.cn-beijing.volces.com/api/v3"
MIDSCENE_MODEL_API_KEY="...."
MIDSCENE_MODEL_NAME="ep-..." # Inference endpoint ID or model name from Volcano Engine
MIDSCENE_USE_DOUBAO_VISION=1
```

**Links**
- [Volcano Engine - Doubao-1.5-thinking-vision-pro](https://www.volcengine.com/docs/82379/1536428)
- [Volcano Engine - Doubao-Seed-1.6-Vision](https://www.volcengine.com/docs/82379/1799865)

<div id="qwen3-vl-or-qwen-25-vl"></div>

### Qwen VL (âœ¨ recommended)

Qwen-VL is an open-source model series released by Alibaba. It provides visual grounding and understanding capabilities, performs excellently in interaction, assertion, and querying, and Qwen3-VL's performance significantly leads other models.

You can find deployed versions of the Qwen series on [Alibaba Cloud](https://help.aliyun.com/zh/model-studio/vision) or [OpenRouter](https://openrouter.ai/qwen), or deploy open-source versions yourself using tools like [Ollama](https://ollama.com/library/qwen3-vl).

Midscene.js supports using the following versions of Qwen models:
* Qwen3-VL series (recommended), including `qwen3-vl-plus` (commercial) and `qwen3-vl` various sizes of open-source versions
* Qwen2.5-VL series

**Configuration for Qwen3-VL models**

Using Alibaba Cloud's `qwen3-vl-plus` model as an example:

```bash
MIDSCENE_MODEL_BASE_URL="https://dashscope.aliyuncs.com/compatible-mode/v1"
MIDSCENE_MODEL_API_KEY="......"
MIDSCENE_MODEL_NAME="qwen3-vl-plus"
MIDSCENE_USE_QWEN3_VL=1 # Note: this parameter cannot be used together with MIDSCENE_USE_QWEN_VL
```

**Configuration for Qwen2.5-VL models**

Using Alibaba Cloud's `qwen-vl-max-latest` model as an example:

```bash
MIDSCENE_MODEL_BASE_URL="https://dashscope.aliyuncs.com/compatible-mode/v1"
MIDSCENE_MODEL_API_KEY="......"
MIDSCENE_MODEL_NAME="qwen-vl-max-latest"
MIDSCENE_USE_QWEN_VL=1 # Note: this parameter cannot be used together with MIDSCENE_USE_QWEN3_VL
```

**Links**
- [Alibaba Cloud - Qwen-VL series](https://help.aliyun.com/zh/model-studio/vision)
- [Qwen on ðŸ¤— HuggingFace](https://huggingface.co/Qwen)
- [Qwen on Github](https://github.com/QwenLM/)
- [Qwen on openrouter.ai](https://openrouter.ai/qwen)

<div id="gemini-25-pro"></div>

### `Gemini-2.5-Pro`

Gemini 2.5 Pro is a proprietary model provided by Google Cloud. Gemini 2.5 Pro is less accurate in UI positioning than Doubao and Qwen models.

When using Gemini-2.5-Pro, you should use the `MIDSCENE_USE_GEMINI=1` configuration to enable Gemini-2.5-Pro mode.

**Configuration**

After applying for an API key on [Google Gemini](https://gemini.google.com/), you can use the following configuration:

```bash
MIDSCENE_MODEL_BASE_URL="https://generativelanguage.googleapis.com/v1beta/openai/"
MIDSCENE_MODEL_API_KEY="......"
MIDSCENE_MODEL_NAME="gemini-2.5-pro-preview-05-06"
MIDSCENE_USE_GEMINI=1
```

**Links**
- [Gemini 2.5 on Google Cloud](https://cloud.google.com/gemini-api/docs/gemini-25-overview)

<div id="ui-tars"></div>

### `UI-TARS`

UI-TARS is a GUI Agent-specific model based on VLM architecture. It takes screenshots as input and performs human-like interactions (such as keyboard and mouse operations), achieving state-of-the-art performance across 10+ GUI benchmarks.
UI-TARS provides different sizes of open-source versions.

With UI-TARS, you can use goal-driven style prompts, such as "Log in with username foo and password bar", and it will plan steps to achieve the goal.

**Configuration**

You can use the deployed `doubao-1.5-ui-tars` on [Volcano Engine](https://volcengine.com).

```bash
MIDSCENE_MODEL_BASE_URL="https://ark.cn-beijing.volces.com/api/v3"
MIDSCENE_MODEL_API_KEY="...."
MIDSCENE_MODEL_NAME="ep-2025..." # Inference endpoint ID or model name from Volcano Engine
MIDSCENE_USE_VLM_UI_TARS=DOUBAO
```

**Limitations**

- **Weak assertion performance**: It may not perform as well as other models in assertion and visual understanding
- **Unstable operation paths**: It may try different paths to achieve goals, so the operation path is not stable on each call.

**About the `MIDSCENE_USE_VLM_UI_TARS` configuration**

The `MIDSCENE_USE_VLM_UI_TARS` configuration is used to specify the UI-TARS version, using one of the following values:
- `1.0` - for model version `1.0`
- `1.5` - for model version `1.5`
- `DOUBAO` - for models deployed on Volcano Engine

**Links**
- [UI-TARS on ðŸ¤— HuggingFace](https://huggingface.co/bytedance-research/UI-TARS-72B-SFT)
- [UI-TARS on GitHub](https://github.com/bytedance/ui-tars)
- [UI-TARS - Model Deployment Guide](https://juniper-switch-f10.notion.site/UI-TARS-Model-Deployment-Guide-17b5350241e280058e98cea60317de71)
- [UI-TARS on Volcengine](https://www.volcengine.com/docs/82379/1536429)


<div id="gpt-4o"></div>
### <del>`GPT-4o`</del>

Starting from version 1.0, Midscene no longer supports using GPT-4o as the UI operation planning model.

For more details and example configurations, please see [Configure Model and Provider](./model-provider).

## FAQ

### How can I check the model's token usage?

By setting `DEBUG=midscene:ai:profile:stats` in the environment variables, you can print the model's usage info and response time.

You can also see your model's usage info in the report file.

### Get an error message "No visual language model (VL model) detected"

Make sure you have configured the VL model correctly, especially the `MIDSCENE_USE_...` config is set correctly.

## More

* To learn more about the model configuration, see [Config Model and Provider](./model-provider)
* [Prompting Tips](./prompting-tips)

<TroubleshootingLLMConnectivity />

## Compatible with Midscene 0.x versions

Some configurations used in Midscene 0.x versions (such as `OPENAI_API_KEY` `OPENAI_BASE_URL` `MIDSCENE_USE_QWEN_VL`) remain compatible in 1.x versions. For details, see [Compatible with Midscene 0.x configurations](./model-provider.mdx).
