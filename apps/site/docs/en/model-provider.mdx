import TroubleshootingLLMConnectivity from './common/troubleshooting-llm-connectivity.mdx';

# Configure model and provider

Midscene uses the OpenAI SDK to call AI services. Using this SDK limits the input and output schema of AI services, but it doesn't mean you can only use OpenAI's services. You can use any model service that supports the same interface (most platforms or tools support this).

In this article, we will show you how to config AI service provider and how to choose a different model. You may read [Choose a model](./choose-a-model) first to learn more about how to choose a model.

## Configs

### Common configs
These are the most common configs, in which `MIDSCENE_MODEL_API_KEY` or `OPENAI_API_KEY` is required.

| Name | Description |
|------|-------------|
| `MIDSCENE_MODEL_API_KEY` | Required (recommended). Your API key (e.g. "sk-abcdefghijklmnopqrstuvwxyz") |
| `MIDSCENE_MODEL_BASE_URL` | Optional (recommended). Custom endpoint URL for API endpoint. Use it to switch to a provider other than OpenAI (e.g. "https://some_service_name.com/v1") |
| `OPENAI_API_KEY` | Deprecated but still compatible. Recommended to use `MIDSCENE_MODEL_API_KEY` |
| `OPENAI_BASE_URL` | Deprecated but still compatible. Recommended to use `MIDSCENE_MODEL_BASE_URL` |
| `MIDSCENE_MODEL_NAME` | Optional. Specify a different model name other than `gpt-4o` |

Extra configs to use `Qwen 2.5 VL` model:

| Name | Description |
|------|-------------|
| `MIDSCENE_USE_QWEN_VL` | Set to "1" to use the adapter of Qwen 2.5 VL model |

Extra configs to use `UI-TARS` model:

| Name | Description |
|------|-------------|
| `MIDSCENE_USE_VLM_UI_TARS` | Version of UI-TARS model, supported values are `1.0` `1.5` `DOUBAO` (volcengine version) |

Extra configs to use `Gemini 2.5 Pro` model:

| Name | Description |
|------|-------------|
| `MIDSCENE_USE_GEMINI` | Set to "1" to use the adapter of Gemini 2.5 Pro model |

For more information about the models, see [Choose a model](./choose-a-model).

### Configure Models by Task Type (Advanced)

Midscene internally categorizes AI tasks into different intent types. You can configure different models for different intents:

- **Insight tasks**: Visual Question Answering (VQA) and Visual Grounding, such as `aiQuery`, `aiLocate`, `aiTap`, etc.
- **Planning tasks**: Automatic planning tasks, such as `aiAct`
- **Default tasks**: Other uncategorized tasks

Each task type can have independent model configurations:

| Task Type | Environment Variable Prefix | Description |
|-----------|---------------------------|-------------|
| Insight | `MIDSCENE_INSIGHT_MODEL_*` | For visual understanding and element location tasks |
| Planning | `MIDSCENE_PLANNING_MODEL_*` | For automatic planning tasks |
| Default | `MIDSCENE_MODEL_*` | Default configuration, used as fallback for other tasks |

Complete configuration options supported by each prefix:

| Configuration | Description |
|--------------|-------------|
| `*_MODEL_NAME` | Model name |
| `*_MODEL_API_KEY` | API key |
| `*_MODEL_BASE_URL` | API endpoint URL |
| `*_MODEL_HTTP_PROXY` | HTTP/HTTPS proxy |
| `*_MODEL_SOCKS_PROXY` | SOCKS proxy |
| `*_MODEL_INIT_CONFIG_JSON` | OpenAI SDK initialization config JSON |
| `*_LOCATOR_MODE` | Locator mode (e.g. `qwen3-vl`, `vlm-ui-tars`, etc.) |

**Example: Configure different models for Insight and Planning tasks**

```bash
# Insight tasks use Qwen-VL model (for visual understanding and location)
export MIDSCENE_INSIGHT_MODEL_NAME="qwen-vl-plus"
export MIDSCENE_INSIGHT_MODEL_API_KEY="sk-insight-key"
export MIDSCENE_INSIGHT_MODEL_BASE_URL="https://dashscope.aliyuncs.com/compatible-mode/v1"
export MIDSCENE_INSIGHT_LOCATOR_MODE="qwen3-vl"

# Planning tasks use GPT-4o model (for task planning)
export MIDSCENE_PLANNING_MODEL_NAME="gpt-4o"
export MIDSCENE_PLANNING_MODEL_API_KEY="sk-planning-key"
export MIDSCENE_PLANNING_MODEL_BASE_URL="https://api.openai.com/v1"
export MIDSCENE_PLANNING_LOCATOR_MODE="qwen3-vl"

# Default configuration (used as fallback)
export MIDSCENE_MODEL_NAME="gpt-4o"
export MIDSCENE_MODEL_API_KEY="sk-default-key"
```

:::tip

If a task type's configuration is not set, Midscene will automatically use the default `MIDSCENE_MODEL_*` configuration. In most cases, you only need to configure the default `MIDSCENE_MODEL_*` variables.

:::

### Advanced configs

Some advanced configs are also supported. Usually you don't need to use them.

| Name | Description |
|------|-------------|
| `MIDSCENE_MODEL_INIT_CONFIG_JSON` | Optional (recommended). Custom JSON config for OpenAI SDK initialization |
| `MIDSCENE_MODEL_HTTP_PROXY` | Optional (recommended). HTTP/HTTPS proxy configuration (e.g. `http://127.0.0.1:8080` or `https://proxy.example.com:8080`). This option has higher priority than `MIDSCENE_MODEL_SOCKS_PROXY` |
| `MIDSCENE_MODEL_SOCKS_PROXY` | Optional (recommended). SOCKS proxy configuration (e.g. "socks5://127.0.0.1:1080") |
| `MIDSCENE_OPENAI_INIT_CONFIG_JSON` | Deprecated but still compatible. Recommended to use `MIDSCENE_MODEL_INIT_CONFIG_JSON` |
| `MIDSCENE_OPENAI_HTTP_PROXY` | Deprecated but still compatible. Recommended to use `MIDSCENE_MODEL_HTTP_PROXY` |
| `MIDSCENE_OPENAI_SOCKS_PROXY` | Deprecated but still compatible. Recommended to use `MIDSCENE_MODEL_SOCKS_PROXY` |
| `MIDSCENE_PREFERRED_LANGUAGE` | Optional. The preferred language for the model response. The default is `Chinese` if the current timezone is GMT+8 and `English` otherwise. |
| `MIDSCENE_REPLANNING_CYCLE_LIMIT` | Optional. The maximum number of replanning cycles, default is 10 |
| `MIDSCENE_MODEL_MAX_TOKENS` | Optional (recommended). Maximum tokens for model response, default is 2048 |
| `OPENAI_MAX_TOKENS` | Deprecated but still compatible. Recommended to use `MIDSCENE_MODEL_MAX_TOKENS` |

### Debug configs

By setting the following configs, you can see more logs for debugging. And also, they will be printed into the `./midscene_run/log` folder.

| Name | Description |
|------|-------------|
| `DEBUG=midscene:ai:profile:stats` | Optional. Set this to print the AI service cost time, token usage, etc. in comma separated format, useful for analysis |
| `DEBUG=midscene:ai:profile:detail` | Optional. Set this to print the AI token usage details |
| `DEBUG=midscene:ai:call` | Optional. Set this to print the AI response details |
| `DEBUG=midscene:android:adb` | Optional. Set this to print the adb command calling details |

## Two ways to configure environment variables

Pick one of the following ways to config environment variables.

### 1. Set environment variables in your system

```bash
# replace by your own
export MIDSCENE_MODEL_API_KEY="sk-abcdefghijklmnopqrstuvwxyz"

# if you are not using the default OpenAI model, you need to config more params
# export MIDSCENE_MODEL_NAME="..."
```

### 2. Set environment variables using dotenv

This is what we used in our [demo project](https://github.com/web-infra-dev/midscene-example).

[Dotenv](https://www.npmjs.com/package/dotenv) is a zero-dependency module that loads environment variables from a `.env` file into `process.env`.

```bash
# install dotenv
npm install dotenv --save
```

Create a `.env` file in your project root directory, and add the following content. There is no need to add `export` before each line.

```
MIDSCENE_MODEL_API_KEY=sk-abcdefghijklmnopqrstuvwxyz
```

Import the dotenv module in your script. It will automatically read the environment variables from the `.env` file.

```typescript
import 'dotenv/config';
```

## Set config by JavaScript

You can also override the config by javascript. Remember to call this before running Midscene codes.

```typescript
import { overrideAIConfig } from "@midscene/web/puppeteer";
// or import { overrideAIConfig } from "@midscene/web/playwright";
// or import { overrideAIConfig } from "@midscene/android";


overrideAIConfig({
  MIDSCENE_MODEL_NAME: "...",
  MIDSCENE_MODEL_BASE_URL: "...", // recommended, use new variable name
  MIDSCENE_MODEL_API_KEY: "...", // recommended, use new variable name
  // ...
});
```

## Example: using `gpt-4o` from OpenAI

Configure the environment variables:

```bash
export MIDSCENE_MODEL_API_KEY="sk-..."
export MIDSCENE_MODEL_BASE_URL="https://endpoint.some_other_provider.com/v1" # config this if you want to use a different endpoint
export MIDSCENE_MODEL_NAME="gpt-4o-2024-11-20" # optional, the default is "gpt-4o"
```

## Example: using `qwen-vl-max-latest` from Aliyun

Configure the environment variables:

```bash
export MIDSCENE_MODEL_API_KEY="sk-..."
export MIDSCENE_MODEL_BASE_URL="https://dashscope.aliyuncs.com/compatible-mode/v1"
export MIDSCENE_MODEL_NAME="qwen-vl-max-latest"
export MIDSCENE_USE_QWEN_VL=1
```


## Example: using `Doubao-1.5-thinking-vision-pro` from Volcano Engine

Configure the environment variables:


```bash
export MIDSCENE_MODEL_BASE_URL="https://ark-cn-beijing.bytedance.net/api/v3"
export MIDSCENE_MODEL_API_KEY="..."
export MIDSCENE_MODEL_NAME='ep-...'
export MIDSCENE_USE_DOUBAO_VISION=1
```

## Example: using `ui-tars-72b-sft` hosted by yourself

Configure the environment variables:

```bash
export MIDSCENE_MODEL_API_KEY="sk-..."
export MIDSCENE_MODEL_BASE_URL="http://localhost:1234/v1"
export MIDSCENE_MODEL_NAME="ui-tars-72b-sft"
export MIDSCENE_USE_VLM_UI_TARS=1
```

## Example: config request headers (like for openrouter)

```bash
export MIDSCENE_MODEL_BASE_URL="https://openrouter.ai/api/v1"
export MIDSCENE_MODEL_API_KEY="..."
export MIDSCENE_MODEL_NAME="..."
export MIDSCENE_OPENAI_INIT_CONFIG_JSON='{"defaultHeaders":{"HTTP-Referer":"...","X-Title":"..."}}'
```

<TroubleshootingLLMConnectivity />
