# Integrate with any interface (preview)

From Midscene v0.28.0, we have launched the feature to integrate with any interface. By inheriting the `AbstractInterface` class and tell Midscene how to interact with the interface, a fully-featured Midscene AI Agent will be created. All these popular tools are included by default: the interaction methods, data extraction methods, the report, the GUI playground, the cli, the yaml script runner, the MCP server...

:::tip This is a preview feature

This feature is still in preview. We have finished the following work

✅ Allow developers to define a class that extends the `AbstractInterface` class

✅ Allow developers to use yaml script to drive the interface

And the next work is

- Allow developers to use the playground

- Allow developers to use the MCP server

:::


## Demo and Community Project

We have prepared a demo project for you to learn how to define your own interface class. It's highly recommended to check it out.

* [Demo Project](https://github.com/web-infra-dev/midscene-example/tree/main/custom-interface) - A simple demo project that shows how to define your own interface class

* [Android (adb) Agent](https://github.com/web-infra-dev/midscene/blob/main/packages/android/src/device.ts) - This is the Android (adb) Agent for Midscene that implements this feature

There are also some community projects that use this feature:

* [midscene-ios](https://github.com/lhuanyu/midscene-ios) - A project driving the "iPhone Mirror" app with Midscene

## Implement your own interface class

### Key concepts

* The `AbstractInterface` class: a predefined abstract class that can connect to the Midscene Agent
* The **action space**: a set of actions that describe the actions that can be performed on the interface. This will affect how the AI model plans the actions and executes them

### Step 1. clone and setup from the demo project

This is the fastest way to get started.

```bash
git clone https://github.com/web-infra-dev/midscene-example.git
cd midscene-example/custom-interface
npm install
npm run build
```

### Step 2. implement your interface class

Define a class that extends the `AbstractInterface` class, and implement the required methods.

You can get the sample implementation from the `./src/device/sample-device.ts` file.

The key methods that you need to implement are:
- `screenshotBase64()`: take a screenshot of the interface and return the base64 string
- `size()`: get the size and dpr of the interface
- `actionSpace()`: define the action space of the interface, which is an array of `DeviceAction` objects. AI model will use these actions to perform the actions.

Use these commands to run the agent:

- `npm run build` to rebuild the agent
- `npm run demo` to run the agent with javascript
- `npm run demo:yaml` to run the agent with yaml script


### Step 3. test the agent with the playground

(coming soon...)

### Step 4. test the MCP server

(still in progress...)

### Step 5. release the npm package, and let your users to use it

Fill the `name` and `version` in the `package.json` file, and then run the following command:

```bash
npm publish
```

A typical usage of your npm package is like this:

```typescript
import { midsceneAgentForSampleDevice } from '...';

const agent = midsceneAgentForSampleDevice({
  foo: 'bar',
});

await agent.aiAction('click the button');
```

## API Reference

### `AbstractInterface` class

```typescript
import { AbstractInterface } from '@midscene/core';
```

`AbstractInterface` is the key class for the agent to control the interface. 

These are the required methods that you need to implement:

- `interfaceType: string`: define a name for the interface
- `screenshotBase64()`: take a screenshot of the interface and return the base64 string with the `'data:image/` prefix
- `size()`: the size and dpr of the interface, which is an object with the `width`, `height`, and `dpr` properties
- `actionSpace()`: the action space of the interface, which is an array of `DeviceAction` objects

These are the optional methods that you can implement:

- `destroy?()`: destroy the interface
- `describe?()`: describe the interface, this may be used for the report and the playground. But it will not be provided to the AI model.
- `beforeInvokeAction?()`: a hook function before invoke an action in action space
- `afterInvokeAction?()`: a hook function after invoke an action

### The action space

To help you easily define the action space, Midscene has provided a set of predefined action spaces for the most common interfaces and devices. And there is also a method to define any custom action.

This is how you can import the utils to define the action space:

```typescript
import {
	type ActionTapParam,
	defineAction,
	defineActionTap,
} from "@midscene/core/device";
```

#### The predefined action space

These are the predefined action spaces for the most common interfaces and devices. You can expose them to the customized interface by implementing the call method of the action.

You can find the parameters of the actions in the type definition of these functions.

* `defineActionTap()`: define the tap action. This is also the function to invoke for the `aiTap` method.
* `defineActionDoubleClick()`: define the double click action
* `defineActionInput()`: define the input action. This is also the function to invoke for the `aiInput` method. This is also the function to invoke for the `aiInput` method.
* `defineActionKeyboardPress()`: define the keyboard press action. This is also the function to invoke for the `aiKeyboardPress` method.
* `defineActionScroll()`: define the scroll action. This is also the function to invoke for the `aiScroll` method.
* `defineActionDragAndDrop()`: define the drag and drop action
* `defineActionLongPress()`: define the long press action
* `defineActionSwipe()`: define the swipe action

#### The custom action space

You can define your own action by using the `defineAction()` function.

API Signature:

```typescript
import { defineAction } from "@midscene/core/device";

defineAction(
  {
    name: string,
    description: string,
    paramSchema: z.ZodType<T>;
    call: (param: z.infer<z.ZodType<T>>) => Promise<void>;
  }
)
```

* `name`: the name of the action, AI model will use this name to invoke the action
* `description`: the description of the action, AI model will use this description to understand what the action is doing
* `paramSchema`: the [Zod](https://www.npmjs.com/package/zod) schema of the parameters of the action, AI model will help to fill the parameters according to this schema
* `call`: the function to invoke the action, you can get the parameters from the `param` parameter which conforms to the `paramSchema`


Example:

```typescript
defineAction({
  name: 'MyAction',
  description: 'My action',
  paramSchema: z.object({
    name: z.string(),
  }),
  call: async (param) => {
    console.log(param.name);
  },
});
```

If you want to get a param about the location of the element, you can use the `getMidsceneLocationSchema()` function to get the specific zod schema.

A more complex example:

```typescript
import { getMidsceneLocationSchema } from "@midscene/core/device";

defineAction({
  name: 'LaunchApp',
  description: 'A an app on screen',
  paramSchema: z.object({
    name: z.string().describe('The name of the app to launch'),
    locate: getMidsceneLocationSchema().describe('The app icon to be launched'),
  }),
  call: async (param) => {
    console.log(`launching app: ${param.name}, located at: ${JSON.stringify(param.locate.center)}`);
  },
});
```


