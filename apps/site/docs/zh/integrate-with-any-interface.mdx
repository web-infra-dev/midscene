import SetupEnv from './common/setup-env.mdx';

# 与任意界面集成（预览特性）

从 Midscene v0.28.0 开始，我们推出了与任意界面集成的功能。通过继承 `AbstractInterface` 类并告诉 Midscene 如何与界面交互，就可以创建一个功能齐全的 Midscene Agent。默认包含以下能力：

- TypeScript 的 GUI 自动化 Agent SDK
- Playground 调试面板
- 通过 yaml 脚本控制界面
- MCP 服务器
- Midscene Agent 的全部能力

请注意：只有具备视觉锚定（visual grounding）能力的模型才能用于控制界面。请阅读文档以[选择合适的模型](./choose-a-model)。

:::tip 预览功能说明
此功能仍在预览阶段。目前我们已经完成了以下工作：

✅ 允许开发者定义继承 `AbstractInterface` 的类

✅ 允许开发者使用 yaml 脚本驱动界面

✅ 允许开发者使用 Playground

接下来将要完成：

- 允许开发者使用 MCP 服务器

:::


## 演示和社区项目

我们已经为你准备了一个演示项目，帮助你学习如何定义自己的界面类。强烈建议你查看一下。

* [演示项目](https://github.com/web-infra-dev/midscene-example/tree/main/custom-interface) - 一个简单的演示项目，展示如何定义自己的界面类

* [Android (adb) Agent](https://github.com/web-infra-dev/midscene/blob/main/packages/android/src/device.ts) - 这是 Midscene Android (adb) Agent，同样依赖此特性实现

还有一些使用此功能的社区项目：

* [midscene-ios](https://github.com/lhuanyu/midscene-ios) - 使用 Midscene 驱动 "iPhone 镜像" 应用的项目


<SetupEnv />

## 实现你自己的界面类

### 关键概念

* `AbstractInterface` 类：一个预定义的抽象类，可以连接到 Midscene 智能体
* **动作空间**：描述可以在界面上执行的动作集合。这将影响 AI 模型如何规划和执行动作

### 步骤 1. 从 demo 项目开始

这是最快的入门方式。

```bash
git clone https://github.com/web-infra-dev/midscene-example.git
cd midscene-example/custom-interface
npm install
npm run build
```

### 步骤 2. 实现你的界面类

定义一个继承 `AbstractInterface` 类的类，并实现所需的方法。

你可以从 [`./src/sample-device.ts`](https://github.com/web-infra-dev/midscene-example/blob/main/custom-interface/src/sample-device.ts) 文件中获取示例实现。下面是你需要特别关注的关键点：

需要实现的关键方法有：
- `screenshotBase64()`、`size()`：帮助 AI 模型获取界面上下文
- `actionSpace()`：一个由 `DeviceAction` 组成的数组，定义了在界面上可以执行的动作。AI 模型将依赖这些动作来执行具体操作。Midscene 已为常见界面与设备提供了预定义动作空间，同时也支持自定义动作。

使用这些命令运行 Agent：

- `npm run build` 重新编译 Agent 代码
- `npm run demo` 使用 JavaScript 运行智能体
- `npm run demo:yaml` 使用 yaml 脚本运行智能体


### 步骤 3. 使用 Playground 测试 Agent

将 Playground 服务附加到 Agent，即可在浏览器中测试。

（即将推出...）

### 步骤 4. 测试 MCP 服务

（正在进行中...）

### 步骤 5. 发布 npm 包，让你的用户使用它

`./index.ts` 文件已经导出了你的 Agent 与界面类。现在可以发布到 npm。

在 `package.json` 文件中填写 `name` 和 `version`，然后运行以下命令：

```bash
npm publish
```

你的 npm 包的典型用法如下：

```typescript
import { midsceneAgentForSampleDevice } from '...';

const agent = midsceneAgentForSampleDevice({
  foo: 'bar',
});

await agent.aiAction('click the button');
```

### 步骤 6. 在 Midscene CLI 和 YAML 脚本中调用你的类

编写一个包含 `interface` 字段的 yaml 脚本来调用你的类：

```yaml
interface:
  module: 'my-pkg-name'
  # export: 'MyDeviceClass' # 如果是具名导出，使用该字段

config:
  output: './data.json'
```

该配置等价于：

```typescript
import MyDeviceClass from 'my-pkg-name';
const device = new MyDeviceClass();
const agent = new Agent(device, {
  output: './data.json',
});
```

YAML 的其他字段与[自动化脚本](./automate-with-scripts-in-yaml.html)文档一致。

## API 参考

### `AbstractInterface` 类

```typescript
import { AbstractInterface } from '@midscene/core';
```

`AbstractInterface` 是智能体控制界面的关键类。

以下是你需要实现的必需方法：

- `interfaceType: string`：为界面定义一个名称（不会提供给 AI 模型）
- `screenshotBase64()`：截取界面的屏幕截图并返回带有 `'data:image/` 前缀的 base64 字符串
- `size()`：界面的大小和 dpr，它是一个具有 `width`、`height` 和 `dpr` 属性的对象
- `actionSpace()`：界面的动作空间，它是一个 `DeviceAction` 对象数组。可使用预定义动作空间，或定义任意自定义动作。

以下是你可以实现的可选方法：

- `destroy?()`：销毁
- `describe?()`：描述这个界面，返回值用于报告和 Playground，但不会提供给 AI 模型
- `beforeInvokeAction?()`：调用动作之前的钩子函数
- `afterInvokeAction?()`：调用动作之后的钩子函数

### 动作空间（Action Space）

动作空间是界面上可执行动作的集合。AI 模型将基于这些动作来计划并执行操作。动作的描述与参数模式都会提供给 AI 模型。

为了帮助你轻松定义动作空间，Midscene 为最常见的界面和设备提供了一组预定义的动作空间，同时也支持定义任意自定义动作。

以下是如何导入工具来定义动作空间：

```typescript
import {
	type ActionTapParam,
	defineAction,
	defineActionTap,
} from "@midscene/core/device";
```

#### 预定义的动作空间

这些是最常见界面和设备的预定义动作空间。你可以通过实现动作的调用方法将它们暴露给定制化界面。

你可以在这些函数的类型定义中找到动作的参数。

* `defineActionTap()`：定义点击动作。这也是调用 `aiTap` 方法的函数。
* `defineActionDoubleClick()`：定义双击动作
* `defineActionInput()`：定义输入动作。这也是调用 `aiInput` 方法的函数。这也是调用 `aiInput` 方法的函数。
* `defineActionKeyboardPress()`：定义键盘按下动作。这也是调用 `aiKeyboardPress` 方法的函数。
* `defineActionScroll()`：定义滚动动作。这也是调用 `aiScroll` 方法的函数。
* `defineActionDragAndDrop()`：定义拖放动作
* `defineActionLongPress()`：定义长按动作
* `defineActionSwipe()`：定义滑动动作

#### 定义一个自定义动作

你可以使用 `defineAction()` 函数定义自己的动作。

API 签名：

```typescript
import { defineAction } from "@midscene/core/device";

defineAction(
  {
    name: string,
    description: string,
    paramSchema: z.ZodType<T>;
    call: (param: z.infer<z.ZodType<T>>) => Promise<void>;
  }
)
```

* `name`：动作的名称，AI 模型将使用此名称调用动作
* `description`：动作的描述，AI 模型将使用此描述来理解动作的作用。对于复杂动作，你可以在这里给出更详细的示例说明
* `paramSchema`：动作参数的 [Zod](https://www.npmjs.com/package/zod) 模式，AI 模型将根据此模式帮助填充参数
* `call`：调用动作的函数，你可以从符合 `paramSchema` 的 `param` 参数中获取参数


示例：

```typescript
defineAction({
  name: 'MyAction',
  description: 'My action',
  paramSchema: z.object({
    name: z.string(),
  }),
  call: async (param) => {
    console.log(param.name);
  },
});
```

如果你想要获取某个元素位置相关的参数，可以使用 `getMidsceneLocationSchema()` 函数获取特定的 zod 模式。

一个更复杂的示例，关于如何定义自定义动作：

```typescript
import { getMidsceneLocationSchema } from "@midscene/core/device";

defineAction({
  name: 'LaunchApp',
  description: 'A an app on screen',
  paramSchema: z.object({
    name: z.string().describe('The name of the app to launch'),
    locate: getMidsceneLocationSchema().describe('The app icon to be launched'),
  }),
  call: async (param) => {
    console.log(`launching app: ${param.name}, ui located at: ${JSON.stringify(param.locate.center)}`);
  },
});
```


## 常见问题（FAQ）

**我可以使用普通的 LLM（如 GPT-4o）来控制界面吗？**

不可以。你必须使用具备视觉锚定能力的模型。具备视觉锚定能力的模型可以在页面上定位目标元素并返回其坐标，这能显著提升自动化的稳定性。

请阅读文档以[选择合适的模型](./choose-a-model)。

**我的 interface-controller 可以在本文档中被推荐吗？**

可以。我们欢迎收集有创意的项目并在本文档中列出它们。

[当你的项目准备好后，给我们提一个 issue](https://github.com/web-infra-dev/midscene/issues)。
